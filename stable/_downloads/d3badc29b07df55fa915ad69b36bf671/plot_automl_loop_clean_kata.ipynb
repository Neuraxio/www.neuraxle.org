{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nUsage of AutoML loop, and hyperparams with sklearn models.\n=============================================================\n\nThis demonstrates how you can build an AutoML loop that finds the best possible sklearn classifier.\nIt also shows you how to add hyperparams to sklearn steps using SKLearnWrapper.\nThis example has been derived and simplified from the following repository: https://github.com/Neuraxio/Kata-Clean-Machine-Learning-From-Dirty-Code\nHere, 2D data is fitted, whereas in the original example 3D (sequential / time series) data is preprocessed and then fitted with the same models. \n\n..\n    Copyright 2019, Neuraxio Inc.\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import shutil\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import RidgeClassifier, LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n\nfrom neuraxle.hyperparams.distributions import Choice, RandInt, Boolean, LogUniform\nfrom neuraxle.hyperparams.space import HyperparameterSpace\nfrom neuraxle.metaopt.auto_ml import AutoML, RandomSearchHyperparameterSelectionStrategy, ValidationSplitter, \\\n    HyperparamsJSONRepository\nfrom neuraxle.metaopt.callbacks import ScoringCallback\nfrom neuraxle.pipeline import Pipeline\nfrom neuraxle.steps.flow import ChooseOneStepOf\nfrom neuraxle.steps.numpy import NumpyRavel\nfrom neuraxle.steps.output_handlers import OutputTransformerWrapper\nfrom neuraxle.steps.sklearn import SKLearnWrapper\n\n\ndef main():\n    # Define classification models, and hyperparams.\n    # See also HyperparameterSpace documentation : https://www.neuraxle.org/stable/api/neuraxle.hyperparams.space.html#neuraxle.hyperparams.space.HyperparameterSpace\n\n    decision_tree_classifier = SKLearnWrapper(\n        DecisionTreeClassifier(), HyperparameterSpace({\n            'criterion': Choice(['gini', 'entropy']),\n            'splitter': Choice(['best', 'random']),\n            'min_samples_leaf': RandInt(2, 5),\n            'min_samples_split': RandInt(1, 3)\n        }))\n\n    extra_tree_classifier = SKLearnWrapper(\n        ExtraTreeClassifier(), HyperparameterSpace({\n            'criterion': Choice(['gini', 'entropy']),\n            'splitter': Choice(['best', 'random']),\n            'min_samples_leaf': RandInt(2, 5),\n            'min_samples_split': RandInt(1, 3)\n        }))\n\n    ridge_classifier = Pipeline([\n        OutputTransformerWrapper(NumpyRavel()),\n        SKLearnWrapper(RidgeClassifier(), HyperparameterSpace({\n            'alpha': Choice([(0.0, 1.0, 10.0), (0.0, 10.0, 100.0)]),\n            'fit_intercept': Boolean(),\n            'normalize': Boolean()\n        }))\n    ]).set_name('RidgeClassifier')\n\n    logistic_regression = Pipeline([\n        OutputTransformerWrapper(NumpyRavel()),\n        SKLearnWrapper(LogisticRegression(), HyperparameterSpace({\n            'C': LogUniform(0.01, 10.0),\n            'fit_intercept': Boolean(),\n            'dual': Boolean(),\n            'penalty': Choice(['l1', 'l2']),\n            'max_iter': RandInt(20, 200)\n        }))\n    ]).set_name('LogisticRegression')\n\n    random_forest_classifier = Pipeline([\n        OutputTransformerWrapper(NumpyRavel()),\n        SKLearnWrapper(RandomForestClassifier(), HyperparameterSpace({\n            'n_estimators': RandInt(50, 600),\n            'criterion': Choice(['gini', 'entropy']),\n            'min_samples_leaf': RandInt(2, 5),\n            'min_samples_split': RandInt(1, 3),\n            'bootstrap': Boolean()\n        }))\n    ]).set_name('RandomForestClassifier')\n\n    # Define a classification pipeline that lets the AutoML loop choose one of the classifier.\n    # See also ChooseOneStepOf documentation : https://www.neuraxle.org/stable/api/neuraxle.steps.flow.html#neuraxle.steps.flow.ChooseOneStepOf\n\n    pipeline = Pipeline([\n        ChooseOneStepOf([\n            decision_tree_classifier,\n            extra_tree_classifier,\n            ridge_classifier,\n            logistic_regression,\n            random_forest_classifier\n        ])\n    ])\n\n    # Create the AutoML loop object.\n    # See also AutoML documentation : https://www.neuraxle.org/stable/api/neuraxle.metaopt.auto_ml.html#neuraxle.metaopt.auto_ml.AutoML\n\n    auto_ml = AutoML(\n        pipeline=pipeline,\n        hyperparams_optimizer=RandomSearchHyperparameterSelectionStrategy(),\n        validation_splitter=ValidationSplitter(test_size=0.20),\n        scoring_callback=ScoringCallback(accuracy_score, higher_score_is_better=True),\n        n_trials=7,\n        epochs=1,\n        hyperparams_repository=HyperparamsJSONRepository(cache_folder='cache'),\n        refit_trial=True,\n    )\n\n    # Load data, and launch AutoML loop !\n\n    X_train, y_train, X_test, y_test = generate_classification_data()\n    auto_ml = auto_ml.fit(X_train, y_train)\n\n    # Get the model from the best trial, and make predictions using predict.\n    # See also predict documentation : https://www.neuraxle.org/stable/api/neuraxle.base.html#neuraxle.base.BaseStep.predict\n\n    best_pipeline = auto_ml.get_best_model()\n    y_pred = best_pipeline.predict(X_test)\n\n    accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n    print(\"Test accuracy score:\", accuracy)\n\n    shutil.rmtree('cache')\n\n\ndef generate_classification_data():\n    data_inputs, expected_outputs = make_classification(\n        n_samples=10000,\n        n_repeated=0,\n        n_classes=3,\n        n_features=4,\n        n_clusters_per_class=1,\n        class_sep=1.5,\n        flip_y=0,\n        weights=[0.5, 0.5, 0.5]\n    )\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        data_inputs,\n        expected_outputs,\n        test_size=0.20\n    )\n\n    return X_train, y_train, X_test, y_test\n\n\nif __name__ == '__main__':\n    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}