{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nBoston Housing Regression with Meta Optimization\n==========================\nThis is an automatic machine learning example. It is more sophisticated than the other simple regression example.\nNot only a pipeline is defined, but also an hyperparameter space is defined for the pipeline. Then, a random search is\nperformed to find the best possible combination of hyperparameters by sampling randomly in the hyperparameter space.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Copyright 2019, The Neuraxle Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_boston\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nfrom neuraxle.hyperparams.distributions import RandInt, LogUniform, Boolean\nfrom neuraxle.hyperparams.space import HyperparameterSpace\nfrom neuraxle.metaopt.random import RandomSearch\nfrom neuraxle.pipeline import Pipeline\nfrom neuraxle.steps.numpy import NumpyTranspose\nfrom neuraxle.steps.sklearn import SKLearnWrapper\nfrom neuraxle.union import AddFeatures, ModelStacking\n\nboston = load_boston()\nX, y = shuffle(boston.data, boston.target, random_state=13)\nX = X.astype(np.float32)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False)\n\n# Note that the hyperparameter spaces are defined here during the pipeline definition, but it could be already set\n# within the classes ar their definition if using custom classes, or also it could be defined after declaring the\n# pipeline using a flat dict or a nested dict.\np = Pipeline([\n    AddFeatures([\n        SKLearnWrapper(\n            PCA(n_components=2),\n            HyperparameterSpace({\"n_components\": RandInt(1, 5)})\n        ),\n        SKLearnWrapper(\n            FastICA(n_components=2),\n            HyperparameterSpace({\"n_components\": RandInt(1, 5)})\n        ),\n    ]),\n    ModelStacking([\n        SKLearnWrapper(\n            GradientBoostingRegressor(),\n            HyperparameterSpace({\n                \"n_estimators\": RandInt(1, 1000), \"max_depth\": RandInt(1, 100), \"learning_rate\": LogUniform(0.001, 1.0)\n            })\n        ),\n        SKLearnWrapper(\n            GradientBoostingRegressor(),\n            HyperparameterSpace({\n                \"n_estimators\": RandInt(1, 1000), \"max_depth\": RandInt(1, 100), \"learning_rate\": LogUniform(0.001, 1.0)\n            })\n        ),\n        SKLearnWrapper(\n            GradientBoostingRegressor(),\n            HyperparameterSpace({\n                \"n_estimators\": RandInt(1, 1000), \"max_depth\": RandInt(1, 100), \"learning_rate\": LogUniform(0.001, 1.0)\n            })\n        ),\n        SKLearnWrapper(\n            KMeans(),\n            HyperparameterSpace({\"n_clusters\": RandInt(2, 50)})\n        ),\n    ],\n        joiner=NumpyTranspose(),\n        judge=SKLearnWrapper(\n            Ridge(),\n            HyperparameterSpace({\"alpha\": LogUniform(0.1, 10.0), \"fit_intercept\": Boolean()})\n        ),\n    )\n])\n\nprint(\"Meta-fitting on train:\")\np = p.meta_fit(X_train, y_train, metastep=RandomSearch(\n    # TODO: cross-validation to avoid overfitting, or evaluate on validation set.\n    n_iter=15, scoring_function=r2_score, higher_score_is_better=True\n))\nprint(\"\")\n\nprint(\"Transforming train and test:\")\ny_train_predicted = p.transform(X_train)\ny_test_predicted = p.transform(X_test)\nprint(\"\")\n\nprint(\"Evaluating transformed train:\")\nscore = r2_score(y_train_predicted, y_train)\nprint('R2 regression score:', score)\nprint(\"\")\n\nprint(\"Evaluating transformed test:\")\nscore = r2_score(y_test_predicted, y_test)\nprint('R2 regression score:', score)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}